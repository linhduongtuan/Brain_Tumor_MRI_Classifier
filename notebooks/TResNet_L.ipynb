{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "#from pytorch_metric_learning import loss\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid-19', 'Normal', 'Pneumonia']\n",
      "{'train': 23028, 'val': 5782}\n",
      "cuda:0\n",
      "{0: 'Covid-19', 1: 'Normal', 2: 'Pneumonia'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Brain/'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "batch_size = 110\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('tresnet_l', num_classes=4, pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.head #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34019875\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "\"\"\"classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 4)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = classifier\"\"\"\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "   \n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs)) #(epoch, num_epochs -1)\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record ACC: {epoch_acc}, previous record acc: {best_acc}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record acc is SAVED: {epoch_acc}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint not found\n",
      "Epoch 0/119\n",
      "----------\n",
      "train Loss: 0.99565659 Acc: 0.73141393\n",
      "val Loss: 0.32016094 Acc: 0.89709443\n",
      "New best model found!\n",
      "New record loss: 0.3201609357794607, previous record loss: inf\n",
      "New record loss is SAVED: 0.3201609357794607\n",
      "\n",
      "Epoch 1/119\n",
      "----------\n",
      "train Loss: 0.38246652 Acc: 0.85526316\n",
      "val Loss: 0.23912624 Acc: 0.92061570\n",
      "New best model found!\n",
      "New record loss: 0.23912624347296524, previous record loss: 0.3201609357794607\n",
      "New record loss is SAVED: 0.23912624347296524\n",
      "\n",
      "Epoch 2/119\n",
      "----------\n",
      "train Loss: 0.33279343 Acc: 0.87519541\n",
      "val Loss: 0.20875521 Acc: 0.93548945\n",
      "New best model found!\n",
      "New record loss: 0.2087552085266275, previous record loss: 0.23912624347296524\n",
      "New record loss is SAVED: 0.2087552085266275\n",
      "\n",
      "Epoch 3/119\n",
      "----------\n",
      "train Loss: 0.30282574 Acc: 0.88535696\n",
      "val Loss: 0.17682431 Acc: 0.94448288\n",
      "New best model found!\n",
      "New record loss: 0.1768243095271524, previous record loss: 0.2087552085266275\n",
      "New record loss is SAVED: 0.1768243095271524\n",
      "\n",
      "Epoch 4/119\n",
      "----------\n",
      "train Loss: 0.27443778 Acc: 0.89777662\n",
      "val Loss: 0.17814918 Acc: 0.94171567\n",
      "\n",
      "Epoch 5/119\n",
      "----------\n",
      "train Loss: 0.25728221 Acc: 0.90402988\n",
      "val Loss: 0.18872981 Acc: 0.94102387\n",
      "\n",
      "Epoch 6/119\n",
      "----------\n",
      "train Loss: 0.24322642 Acc: 0.90841584\n",
      "val Loss: 0.14529500 Acc: 0.95537876\n",
      "New best model found!\n",
      "New record loss: 0.1452950039928098, previous record loss: 0.1768243095271524\n",
      "New record loss is SAVED: 0.1452950039928098\n",
      "\n",
      "Epoch 7/119\n",
      "----------\n",
      "train Loss: 0.23105500 Acc: 0.91545076\n",
      "val Loss: 0.15012727 Acc: 0.95347631\n",
      "\n",
      "Epoch 8/119\n",
      "----------\n",
      "train Loss: 0.21661189 Acc: 0.92148689\n",
      "val Loss: 0.14315850 Acc: 0.95485991\n",
      "New best model found!\n",
      "New record loss: 0.14315849726627294, previous record loss: 0.1452950039928098\n",
      "New record loss is SAVED: 0.14315849726627294\n",
      "\n",
      "Epoch 9/119\n",
      "----------\n",
      "train Loss: 0.21201628 Acc: 0.92231197\n",
      "val Loss: 0.14762418 Acc: 0.95555171\n",
      "\n",
      "Epoch 10/119\n",
      "----------\n",
      "train Loss: 0.19911575 Acc: 0.92600313\n",
      "val Loss: 0.14301947 Acc: 0.95935662\n",
      "New best model found!\n",
      "New record loss: 0.1430194659325459, previous record loss: 0.14315849726627294\n",
      "New record loss is SAVED: 0.1430194659325459\n",
      "\n",
      "Epoch 11/119\n",
      "----------\n",
      "train Loss: 0.19461121 Acc: 0.92808755\n",
      "val Loss: 0.12216349 Acc: 0.96368039\n",
      "New best model found!\n",
      "New record loss: 0.12216348796361295, previous record loss: 0.1430194659325459\n",
      "New record loss is SAVED: 0.12216348796361295\n",
      "\n",
      "Epoch 12/119\n",
      "----------\n",
      "train Loss: 0.18694314 Acc: 0.93017196\n",
      "val Loss: 0.11706898 Acc: 0.96143203\n",
      "New best model found!\n",
      "New record loss: 0.11706898347012355, previous record loss: 0.12216348796361295\n",
      "New record loss is SAVED: 0.11706898347012355\n",
      "\n",
      "Epoch 13/119\n",
      "----------\n",
      "train Loss: 0.17729466 Acc: 0.93486191\n",
      "val Loss: 0.11269996 Acc: 0.96662055\n",
      "New best model found!\n",
      "New record loss: 0.11269995923769677, previous record loss: 0.11706898347012355\n",
      "New record loss is SAVED: 0.11269995923769677\n",
      "\n",
      "Epoch 14/119\n",
      "----------\n",
      "train Loss: 0.16700343 Acc: 0.93911760\n",
      "val Loss: 0.11424950 Acc: 0.96662055\n",
      "\n",
      "Epoch 15/119\n",
      "----------\n",
      "train Loss: 0.17379481 Acc: 0.93694633\n",
      "val Loss: 0.10402580 Acc: 0.97007956\n",
      "New best model found!\n",
      "New record loss: 0.10402580241494548, previous record loss: 0.11269995923769677\n",
      "New record loss is SAVED: 0.10402580241494548\n",
      "\n",
      "Epoch 16/119\n",
      "----------\n",
      "train Loss: 0.16268556 Acc: 0.94068091\n",
      "val Loss: 0.11536357 Acc: 0.96696645\n",
      "\n",
      "Epoch 17/119\n",
      "----------\n",
      "train Loss: 0.16553550 Acc: 0.93937815\n",
      "val Loss: 0.10222764 Acc: 0.97111726\n",
      "New best model found!\n",
      "New record loss: 0.10222763752172415, previous record loss: 0.10402580241494548\n",
      "New record loss is SAVED: 0.10222763752172415\n",
      "\n",
      "Epoch 18/119\n",
      "----------\n",
      "train Loss: 0.15841717 Acc: 0.94215737\n",
      "val Loss: 0.10697621 Acc: 0.97319267\n",
      "\n",
      "Epoch 19/119\n",
      "----------\n",
      "train Loss: 0.14802380 Acc: 0.94680389\n",
      "val Loss: 0.11536748 Acc: 0.96800415\n",
      "\n",
      "Epoch 20/119\n",
      "----------\n",
      "train Loss: 0.15088457 Acc: 0.94484975\n",
      "val Loss: 0.09831855 Acc: 0.97284677\n",
      "New best model found!\n",
      "New record loss: 0.09831855470454433, previous record loss: 0.10222763752172415\n",
      "New record loss is SAVED: 0.09831855470454433\n",
      "\n",
      "Epoch 21/119\n",
      "----------\n",
      "train Loss: 0.14351661 Acc: 0.94815008\n",
      "val Loss: 0.10444724 Acc: 0.97007956\n",
      "\n",
      "Epoch 22/119\n",
      "----------\n",
      "train Loss: 0.13872113 Acc: 0.94984367\n",
      "val Loss: 0.11467795 Acc: 0.97094431\n",
      "\n",
      "Epoch 23/119\n",
      "----------\n",
      "train Loss: 0.13779548 Acc: 0.94936599\n",
      "val Loss: 0.10988443 Acc: 0.97232791\n",
      "\n",
      "Epoch 24/119\n",
      "----------\n",
      "train Loss: 0.13294418 Acc: 0.95227549\n",
      "val Loss: 0.11081974 Acc: 0.97180906\n",
      "\n",
      "Epoch 25/119\n",
      "----------\n",
      "train Loss: 0.13329140 Acc: 0.95101615\n",
      "val Loss: 0.11676475 Acc: 0.97301972\n",
      "\n",
      "Epoch 26/119\n",
      "----------\n",
      "train Loss: 0.13167351 Acc: 0.95357825\n",
      "val Loss: 0.11695240 Acc: 0.97198201\n",
      "\n",
      "Epoch 27/119\n",
      "----------\n",
      "train Loss: 0.12554331 Acc: 0.95379538\n",
      "val Loss: 0.11254493 Acc: 0.97336562\n",
      "\n",
      "Epoch 28/119\n",
      "----------\n",
      "train Loss: 0.12489825 Acc: 0.95501129\n",
      "val Loss: 0.10849493 Acc: 0.97405742\n",
      "\n",
      "Epoch 29/119\n",
      "----------\n",
      "train Loss: 0.12095911 Acc: 0.95670488\n",
      "val Loss: 0.10495523 Acc: 0.97613283\n",
      "\n",
      "Epoch 30/119\n",
      "----------\n",
      "train Loss: 0.12213121 Acc: 0.95657460\n",
      "val Loss: 0.11787613 Acc: 0.97094431\n",
      "\n",
      "Epoch 31/119\n",
      "----------\n",
      "train Loss: 0.11908021 Acc: 0.95614035\n",
      "val Loss: 0.10642794 Acc: 0.97336562\n",
      "\n",
      "Epoch 32/119\n",
      "----------\n",
      "train Loss: 0.12084091 Acc: 0.95687858\n",
      "val Loss: 0.10679929 Acc: 0.97336562\n",
      "\n",
      "Epoch 33/119\n",
      "----------\n",
      "train Loss: 0.12141199 Acc: 0.95444676\n",
      "val Loss: 0.10892614 Acc: 0.97077136\n",
      "\n",
      "Epoch 34/119\n",
      "----------\n",
      "train Loss: 0.11312737 Acc: 0.95770367\n",
      "val Loss: 0.11473400 Acc: 0.97215496\n",
      "\n",
      "Epoch 35/119\n",
      "----------\n",
      "train Loss: 0.11376680 Acc: 0.95900643\n",
      "val Loss: 0.12565745 Acc: 0.97215496\n",
      "\n",
      "Epoch 36/119\n",
      "----------\n",
      "train Loss: 0.10904862 Acc: 0.96061317\n",
      "val Loss: 0.10818469 Acc: 0.97665168\n",
      "\n",
      "Epoch 37/119\n",
      "----------\n",
      "train Loss: 0.11079586 Acc: 0.95926698\n",
      "val Loss: 0.10124032 Acc: 0.97578692\n",
      "\n",
      "Epoch 38/119\n",
      "----------\n",
      "train Loss: 0.10841284 Acc: 0.96087372\n",
      "val Loss: 0.10989224 Acc: 0.97353857\n",
      "\n",
      "Epoch 39/119\n",
      "----------\n",
      "train Loss: 0.10967467 Acc: 0.96091714\n",
      "val Loss: 0.11602676 Acc: 0.97353857\n",
      "\n",
      "Epoch 40/119\n",
      "----------\n",
      "train Loss: 0.10183972 Acc: 0.96400035\n",
      "val Loss: 0.11305971 Acc: 0.97544102\n",
      "\n",
      "Epoch 41/119\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Brain/weights/TResNet_Large.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 300,\n",
    "                                                 checkpoint = None #torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
